_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 448, 448, 12)      0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 446, 446, 64)      6976      
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 446, 446, 64)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 222, 222, 64)      36928     
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 222, 222, 64)      0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 222, 222, 64)      256       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 220, 220, 128)     73856     
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 220, 220, 128)     0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 220, 220, 128)     512       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 109, 109, 128)     147584    
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 109, 109, 128)     0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 109, 109, 128)     512       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 107, 107, 256)     295168    
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 107, 107, 256)     0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 107, 107, 256)     1024      
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 53, 53, 256)       590080    
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 53, 53, 256)       0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 53, 53, 256)       1024      
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 51, 51, 512)       1180160   
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 51, 51, 512)       0         
_________________________________________________________________
batch_normalization_6 (Batch (None, 51, 51, 512)       2048      
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 25, 25, 512)       2359808   
_________________________________________________________________
leaky_re_lu_8 (LeakyReLU)    (None, 25, 25, 512)       0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 25, 25, 512)       2048      
_________________________________________________________________
dense_1 (Dense)              (None, 25, 25, 1024)      525312    
_________________________________________________________________
leaky_re_lu_9 (LeakyReLU)    (None, 25, 25, 1024)      0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 640000)            0         
_________________________________________________________________
dense_4 (Dense)              (None, 256)               163840256 
_________________________________________________________________
leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 257       
=================================================================
Total params: 169,063,809
Trainable params: 169,060,097
Non-trainable params: 3,712
_________________________________________________________________
class 00: #TP=1796367, #FP=113311, #FN=27513, IoU=0.927
class 01: #TP=4699396, #FP=274764, #FN=516532, IoU=0.856
class 02: #TP= 10103, #FP= 37408, #FN=133988, IoU=0.056
class 03: #TP=5751609, #FP=117810, #FN=151051, IoU=0.955
class 04: #TP=1653214, #FP=177229, #FN=140365, IoU=0.839
class 05: #TP=3160278, #FP=254029, #FN=143274, IoU=0.888
class 06: #TP= 97120, #FP= 55828, #FN=118340, IoU=0.358
class 07: #TP=503447, #FP=234154, #FN=122115, IoU=0.586
class 08: #TP=326290, #FP=162491, #FN=49042, IoU=0.607
class 09: #TP= 74735, #FP=133473, #FN=72751, IoU=0.266
class 10: #TP=231868, #FP= 45607, #FN=213334, IoU=0.472
class 11: #TP=113897, #FP=246676, #FN=164475, IoU=0.217
_________________
Mean IoU: 0.586
check point!
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 448, 448, 12)      0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 446, 446, 64)      6976      
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 446, 446, 64)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 222, 222, 64)      36928     
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 222, 222, 64)      0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 222, 222, 64)      256       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 220, 220, 128)     73856     
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 220, 220, 128)     0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 220, 220, 128)     512       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 109, 109, 128)     147584    
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 109, 109, 128)     0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 109, 109, 128)     512       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 107, 107, 256)     295168    
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 107, 107, 256)     0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 107, 107, 256)     1024      
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 53, 53, 256)       590080    
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 53, 53, 256)       0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 53, 53, 256)       1024      
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 51, 51, 512)       1180160   
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 51, 51, 512)       0         
_________________________________________________________________
batch_normalization_6 (Batch (None, 51, 51, 512)       2048      
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 25, 25, 512)       2359808   
_________________________________________________________________
leaky_re_lu_8 (LeakyReLU)    (None, 25, 25, 512)       0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 25, 25, 512)       2048      
_________________________________________________________________
dense_1 (Dense)              (None, 25, 25, 1024)      525312    
_________________________________________________________________
leaky_re_lu_9 (LeakyReLU)    (None, 25, 25, 1024)      0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 640000)            0         
_________________________________________________________________
dense_4 (Dense)              (None, 256)               163840256 
_________________________________________________________________
leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 257       
=================================================================
Total params: 169,063,809
Trainable params: 169,060,097
Non-trainable params: 3,712
_________________________________________________________________
class 00: #TP=1796367, #FP=113311, #FN=27513, IoU=0.927
class 01: #TP=4699396, #FP=274764, #FN=516532, IoU=0.856
class 02: #TP= 10103, #FP= 37408, #FN=133988, IoU=0.056
class 03: #TP=5751609, #FP=117810, #FN=151051, IoU=0.955
class 04: #TP=1653214, #FP=177229, #FN=140365, IoU=0.839
class 05: #TP=3160278, #FP=254029, #FN=143274, IoU=0.888
class 06: #TP= 97120, #FP= 55828, #FN=118340, IoU=0.358
class 07: #TP=503447, #FP=234154, #FN=122115, IoU=0.586
class 08: #TP=326290, #FP=162491, #FN=49042, IoU=0.607
class 09: #TP= 74735, #FP=133473, #FN=72751, IoU=0.266
class 10: #TP=231868, #FP= 45607, #FN=213334, IoU=0.472
class 11: #TP=113897, #FP=246676, #FN=164475, IoU=0.217
_________________
Mean IoU: 0.586
check point!
0 [D loss: 1.313648, acc.: 50.00%] [G loss: 1.066539, acc.: 50.00%]
0 [D loss: 1.434087, acc.: 25.00%] [G loss: 1.183594, acc.: 50.00%]
0 [D loss: 1.267589, acc.: 25.00%] [G loss: 1.212952, acc.: 50.00%]
0 [D loss: 1.167651, acc.: 75.00%] [G loss: 1.080526, acc.: 100.00%]
0 [D loss: 1.060483, acc.: 50.00%] [G loss: 1.528399, acc.: 50.00%]
0 [D loss: 1.196199, acc.: 50.00%] [G loss: 1.240292, acc.: 50.00%]
0 [D loss: 1.309404, acc.: 25.00%] [G loss: 1.235736, acc.: 50.00%]
0 [D loss: 1.231055, acc.: 75.00%] [G loss: 1.459239, acc.: 0.00%]
0 [D loss: 1.099495, acc.: 75.00%] [G loss: 1.080298, acc.: 100.00%]
0 [D loss: 1.211671, acc.: 50.00%] [G loss: 1.315447, acc.: 50.00%]
0 [D loss: 1.266172, acc.: 50.00%] [G loss: 1.364484, acc.: 50.00%]
0 [D loss: 1.162900, acc.: 50.00%] [G loss: 1.117514, acc.: 100.00%]
0 [D loss: 1.295013, acc.: 25.00%] [G loss: 1.119497, acc.: 50.00%]
0 [D loss: 1.297125, acc.: 25.00%] [G loss: 1.164533, acc.: 50.00%]
0 [D loss: 1.357709, acc.: 25.00%] [G loss: 1.324680, acc.: 0.00%]
0 [D loss: 1.452938, acc.: 50.00%] [G loss: 1.482455, acc.: 50.00%]
0 [D loss: 1.314003, acc.: 50.00%] [G loss: 1.105222, acc.: 100.00%]
0 [D loss: 1.374151, acc.: 50.00%] [G loss: 1.051597, acc.: 100.00%]
0 [D loss: 1.361780, acc.: 25.00%] [G loss: 1.030940, acc.: 100.00%]
0 [D loss: 1.369298, acc.: 25.00%] [G loss: 1.321132, acc.: 50.00%]
0 [D loss: 1.332509, acc.: 25.00%] [G loss: 1.290686, acc.: 50.00%]
0 [D loss: 1.285113, acc.: 50.00%] [G loss: 1.438719, acc.: 0.00%]
0 [D loss: 1.241808, acc.: 50.00%] [G loss: 1.306450, acc.: 0.00%]
0 [D loss: 1.297242, acc.: 25.00%] [G loss: 1.119062, acc.: 50.00%]
0 [D loss: 1.324542, acc.: 75.00%] [G loss: 1.999772, acc.: 0.00%]
0 [D loss: 1.258330, acc.: 50.00%] [G loss: 1.248127, acc.: 50.00%]
0 [D loss: 1.039547, acc.: 75.00%] [G loss: 1.586590, acc.: 50.00%]
0 [D loss: 1.298418, acc.: 25.00%] [G loss: 1.341798, acc.: 50.00%]
0 [D loss: 1.231817, acc.: 75.00%] [G loss: 1.532241, acc.: 0.00%]
0 [D loss: 1.269113, acc.: 25.00%] [G loss: 1.120128, acc.: 100.00%]
0 [D loss: 1.123872, acc.: 50.00%] [G loss: 1.357051, acc.: 50.00%]
0 [D loss: 1.183945, acc.: 50.00%] [G loss: 1.103778, acc.: 100.00%]
0 [D loss: 1.161460, acc.: 50.00%] [G loss: 1.315125, acc.: 0.00%]
0 [D loss: 1.006382, acc.: 75.00%] [G loss: 1.658408, acc.: 0.00%]
0 [D loss: 1.248677, acc.: 50.00%] [G loss: 0.974080, acc.: 100.00%]
0 [D loss: 1.150710, acc.: 75.00%] [G loss: 1.235518, acc.: 50.00%]
0 [D loss: 1.062371, acc.: 75.00%] [G loss: 1.345174, acc.: 50.00%]
0 [D loss: 1.151829, acc.: 50.00%] [G loss: 1.191117, acc.: 50.00%]
0 [D loss: 1.166551, acc.: 75.00%] [G loss: 1.470863, acc.: 0.00%]
0 [D loss: 1.200047, acc.: 75.00%] [G loss: 1.126328, acc.: 50.00%]
0 [D loss: 1.255314, acc.: 50.00%] [G loss: 1.121015, acc.: 50.00%]
0 [D loss: 1.573599, acc.: 50.00%] [G loss: 1.403159, acc.: 50.00%]
0 [D loss: 1.226922, acc.: 50.00%] [G loss: 1.708096, acc.: 0.00%]
0 [D loss: 1.306778, acc.: 0.00%] [G loss: 1.135882, acc.: 100.00%]
0 [D loss: 1.199078, acc.: 50.00%] [G loss: 1.433824, acc.: 0.00%]
0 [D loss: 1.428656, acc.: 50.00%] [G loss: 1.835758, acc.: 0.00%]
0 [D loss: 1.241978, acc.: 50.00%] [G loss: 1.949384, acc.: 0.00%]
0 [D loss: 1.220027, acc.: 75.00%] [G loss: 1.715158, acc.: 0.00%]
0 [D loss: 1.393619, acc.: 25.00%] [G loss: 1.294443, acc.: 50.00%]
0 [D loss: 1.377235, acc.: 25.00%] [G loss: 1.135853, acc.: 50.00%]
0 [D loss: 1.190387, acc.: 75.00%] [G loss: 1.462131, acc.: 50.00%]
0 [D loss: 1.264195, acc.: 75.00%] [G loss: 1.076298, acc.: 100.00%]
0 [D loss: 1.460011, acc.: 25.00%] [G loss: 0.889432, acc.: 100.00%]
0 [D loss: 1.166579, acc.: 50.00%] [G loss: 1.548181, acc.: 0.00%]
0 [D loss: 1.317643, acc.: 50.00%] [G loss: 1.339369, acc.: 50.00%]
0 [D loss: 1.304053, acc.: 25.00%] [G loss: 1.283077, acc.: 50.00%]
0 [D loss: 1.302159, acc.: 50.00%] [G loss: 0.989571, acc.: 100.00%]
0 [D loss: 1.000528, acc.: 75.00%] [G loss: 1.346107, acc.: 50.00%]
0 [D loss: 1.162752, acc.: 50.00%] [G loss: 1.306976, acc.: 50.00%]
0 [D loss: 1.126837, acc.: 50.00%] [G loss: 1.152763, acc.: 100.00%]
0 [D loss: 1.384925, acc.: 50.00%] [G loss: 1.038328, acc.: 100.00%]
0 [D loss: 1.434705, acc.: 25.00%] [G loss: 1.207272, acc.: 50.00%]
0 [D loss: 1.416887, acc.: 25.00%] [G loss: 1.209711, acc.: 50.00%]
0 [D loss: 1.240497, acc.: 50.00%] [G loss: 1.473451, acc.: 0.00%]
0 [D loss: 1.084336, acc.: 50.00%] [G loss: 1.113523, acc.: 100.00%]
0 [D loss: 1.340771, acc.: 25.00%] [G loss: 1.245814, acc.: 50.00%]
0 [D loss: 0.940802, acc.: 100.00%] [G loss: 1.464191, acc.: 0.00%]
0 [D loss: 1.208511, acc.: 75.00%] [G loss: 1.599018, acc.: 50.00%]
0 [D loss: 1.189572, acc.: 75.00%] [G loss: 1.193362, acc.: 50.00%]
0 [D loss: 1.093012, acc.: 100.00%] [G loss: 1.220233, acc.: 50.00%]
0 [D loss: 1.234488, acc.: 75.00%] [G loss: 1.662341, acc.: 0.00%]
0 [D loss: 1.310288, acc.: 50.00%] [G loss: 1.608842, acc.: 0.00%]
0 [D loss: 1.163280, acc.: 50.00%] [G loss: 1.011019, acc.: 100.00%]
0 [D loss: 1.044735, acc.: 75.00%] [G loss: 1.305787, acc.: 50.00%]
0 [D loss: 1.355136, acc.: 25.00%] [G loss: 1.563395, acc.: 0.00%]
0 [D loss: 1.000694, acc.: 75.00%] [G loss: 1.286638, acc.: 50.00%]
0 [D loss: 1.287638, acc.: 75.00%] [G loss: 1.383269, acc.: 50.00%]
0 [D loss: 1.282249, acc.: 25.00%] [G loss: 1.586249, acc.: 0.00%]
0 [D loss: 1.053854, acc.: 50.00%] [G loss: 1.126343, acc.: 100.00%]
0 [D loss: 1.168020, acc.: 75.00%] [G loss: 1.194301, acc.: 50.00%]
0 [D loss: 1.162590, acc.: 75.00%] [G loss: 1.870909, acc.: 0.00%]
0 [D loss: 1.098654, acc.: 75.00%] [G loss: 1.084715, acc.: 50.00%]
0 [D loss: 1.174869, acc.: 75.00%] [G loss: 1.375216, acc.: 50.00%]
0 [D loss: 1.239598, acc.: 50.00%] [G loss: 1.397280, acc.: 0.00%]
0 [D loss: 1.117428, acc.: 75.00%] [G loss: 1.244889, acc.: 50.00%]
0 [D loss: 1.137652, acc.: 75.00%] [G loss: 1.102235, acc.: 100.00%]
0 [D loss: 1.072646, acc.: 75.00%] [G loss: 1.127031, acc.: 50.00%]
0 [D loss: 1.306597, acc.: 50.00%] [G loss: 0.925946, acc.: 100.00%]
0 [D loss: 1.199289, acc.: 50.00%] [G loss: 1.226778, acc.: 50.00%]
0 [D loss: 0.972595, acc.: 100.00%] [G loss: 1.297471, acc.: 0.00%]
0 [D loss: 1.133554, acc.: 50.00%] [G loss: 1.321477, acc.: 50.00%]
0 [D loss: 1.059691, acc.: 75.00%] [G loss: 1.542912, acc.: 50.00%]
0 [D loss: 1.095583, acc.: 75.00%] [G loss: 1.537226, acc.: 0.00%]
0 [D loss: 0.987419, acc.: 100.00%] [G loss: 1.083671, acc.: 100.00%]
0 [D loss: 1.292719, acc.: 75.00%] [G loss: 1.568011, acc.: 0.00%]
0 [D loss: 1.183102, acc.: 50.00%] [G loss: 1.355399, acc.: 0.00%]
0 [D loss: 1.042459, acc.: 75.00%] [G loss: 1.334514, acc.: 50.00%]
0 [D loss: 0.935615, acc.: 100.00%] [G loss: 1.245906, acc.: 50.00%]
0 [D loss: 1.393513, acc.: 25.00%] [G loss: 1.427765, acc.: 50.00%]
0 [D loss: 1.303170, acc.: 25.00%] [G loss: 1.266985, acc.: 50.00%]
0 [D loss: 1.192265, acc.: 75.00%] [G loss: 1.538404, acc.: 0.00%]
0 [D loss: 1.161536, acc.: 50.00%] [G loss: 1.282945, acc.: 50.00%]
0 [D loss: 1.182369, acc.: 50.00%] [G loss: 1.276511, acc.: 50.00%]
0 [D loss: 1.203920, acc.: 50.00%] [G loss: 1.823793, acc.: 0.00%]
0 [D loss: 1.083545, acc.: 75.00%] [G loss: 1.162915, acc.: 50.00%]
0 [D loss: 1.229562, acc.: 75.00%] [G loss: 1.171586, acc.: 100.00%]
0 [D loss: 1.047291, acc.: 75.00%] [G loss: 1.662126, acc.: 0.00%]
0 [D loss: 1.311226, acc.: 50.00%] [G loss: 1.828161, acc.: 0.00%]
0 [D loss: 1.189419, acc.: 50.00%] [G loss: 1.599007, acc.: 0.00%]
0 [D loss: 1.273453, acc.: 25.00%] [G loss: 1.080762, acc.: 50.00%]
0 [D loss: 1.160376, acc.: 75.00%] [G loss: 0.915215, acc.: 100.00%]
0 [D loss: 1.095667, acc.: 75.00%] [G loss: 1.284111, acc.: 50.00%]
0 [D loss: 1.148306, acc.: 50.00%] [G loss: 1.377804, acc.: 50.00%]
0 [D loss: 1.107128, acc.: 50.00%] [G loss: 1.083961, acc.: 100.00%]
0 [D loss: 0.953991, acc.: 100.00%] [G loss: 1.218466, acc.: 50.00%]
0 [D loss: 1.160454, acc.: 75.00%] [G loss: 1.066884, acc.: 50.00%]
0 [D loss: 1.145541, acc.: 75.00%] [G loss: 1.005116, acc.: 100.00%]
0 [D loss: 0.956716, acc.: 100.00%] [G loss: 1.543792, acc.: 0.00%]
0 [D loss: 1.210452, acc.: 75.00%] [G loss: 1.551194, acc.: 0.00%]
0 [D loss: 1.092769, acc.: 75.00%] [G loss: 1.050822, acc.: 50.00%]
0 [D loss: 1.228389, acc.: 50.00%] [G loss: 1.313371, acc.: 0.00%]
0 [D loss: 1.277572, acc.: 50.00%] [G loss: 1.290756, acc.: 50.00%]
0 [D loss: 1.089706, acc.: 50.00%] [G loss: 1.278625, acc.: 50.00%]
0 [D loss: 1.024421, acc.: 75.00%] [G loss: 1.727904, acc.: 0.00%]
0 [D loss: 1.169777, acc.: 50.00%] [G loss: 1.190400, acc.: 50.00%]
0 [D loss: 1.267735, acc.: 25.00%] [G loss: 1.224829, acc.: 50.00%]
0 [D loss: 1.057157, acc.: 75.00%] [G loss: 1.481938, acc.: 0.00%]
0 [D loss: 1.060231, acc.: 50.00%] [G loss: 1.884556, acc.: 50.00%]
0 [D loss: 1.170743, acc.: 50.00%] [G loss: 1.295382, acc.: 50.00%]
0 [D loss: 1.225741, acc.: 50.00%] [G loss: 1.238433, acc.: 50.00%]
0 [D loss: 1.184242, acc.: 75.00%] [G loss: 1.429613, acc.: 0.00%]
0 [D loss: 1.197711, acc.: 50.00%] [G loss: 1.347740, acc.: 50.00%]
0 [D loss: 1.205359, acc.: 50.00%] [G loss: 1.431231, acc.: 50.00%]
0 [D loss: 1.068402, acc.: 75.00%] [G loss: 1.691365, acc.: 0.00%]
0 [D loss: 1.286860, acc.: 50.00%] [G loss: 1.568796, acc.: 50.00%]
0 [D loss: 1.238879, acc.: 25.00%] [G loss: 1.613919, acc.: 50.00%]
0 [D loss: 1.232056, acc.: 50.00%] [G loss: 1.206072, acc.: 50.00%]
0 [D loss: 1.018523, acc.: 100.00%] [G loss: 1.739177, acc.: 0.00%]
0 [D loss: 1.150972, acc.: 75.00%] [G loss: 1.751152, acc.: 50.00%]
0 [D loss: 1.162496, acc.: 50.00%] [G loss: 1.578285, acc.: 0.00%]
0 [D loss: 1.082781, acc.: 100.00%] [G loss: 1.550609, acc.: 0.00%]
0 [D loss: 1.305257, acc.: 50.00%] [G loss: 1.242366, acc.: 50.00%]
0 [D loss: 0.849751, acc.: 100.00%] [G loss: 1.624332, acc.: 0.00%]
0 [D loss: 1.282229, acc.: 50.00%] [G loss: 1.631528, acc.: 0.00%]
0 [D loss: 1.048680, acc.: 75.00%] [G loss: 1.389360, acc.: 0.00%]
0 [D loss: 1.090101, acc.: 75.00%] [G loss: 1.198748, acc.: 50.00%]
0 [D loss: 1.261559, acc.: 50.00%] [G loss: 1.067774, acc.: 50.00%]
0 [D loss: 1.125041, acc.: 75.00%] [G loss: 1.695530, acc.: 0.00%]
0 [D loss: 1.107301, acc.: 75.00%] [G loss: 1.078303, acc.: 50.00%]
0 [D loss: 0.972667, acc.: 100.00%] [G loss: 1.403341, acc.: 0.00%]
0 [D loss: 1.166173, acc.: 50.00%] [G loss: 1.210308, acc.: 50.00%]
0 [D loss: 1.108752, acc.: 50.00%] [G loss: 1.164119, acc.: 100.00%]
0 [D loss: 1.076341, acc.: 75.00%] [G loss: 1.659918, acc.: 0.00%]
0 [D loss: 1.107631, acc.: 75.00%] [G loss: 1.388784, acc.: 0.00%]
0 [D loss: 0.990213, acc.: 100.00%] [G loss: 1.457879, acc.: 50.00%]
0 [D loss: 1.024034, acc.: 75.00%] [G loss: 1.009800, acc.: 100.00%]
0 [D loss: 0.989631, acc.: 75.00%] [G loss: 1.465390, acc.: 0.00%]
0 [D loss: 1.044322, acc.: 75.00%] [G loss: 1.670415, acc.: 0.00%]
0 [D loss: 1.125891, acc.: 75.00%] [G loss: 1.459618, acc.: 50.00%]
0 [D loss: 0.963506, acc.: 100.00%] [G loss: 1.511302, acc.: 0.00%]
0 [D loss: 0.971395, acc.: 100.00%] [G loss: 1.229904, acc.: 50.00%]
0 [D loss: 1.060093, acc.: 75.00%] [G loss: 1.286252, acc.: 0.00%]
0 [D loss: 0.885355, acc.: 100.00%] [G loss: 1.537933, acc.: 0.00%]
0 [D loss: 1.201770, acc.: 25.00%] [G loss: 1.302451, acc.: 0.00%]
0 [D loss: 0.915848, acc.: 100.00%] [G loss: 1.517565, acc.: 0.00%]
0 [D loss: 1.229723, acc.: 75.00%] [G loss: 1.268879, acc.: 0.00%]
0 [D loss: 1.188308, acc.: 50.00%] [G loss: 1.245143, acc.: 50.00%]
0 [D loss: 1.280710, acc.: 25.00%] [G loss: 1.177390, acc.: 50.00%]
0 [D loss: 1.241188, acc.: 25.00%] [G loss: 1.291510, acc.: 50.00%]
0 [D loss: 1.138796, acc.: 75.00%] [G loss: 1.035458, acc.: 100.00%]
0 [D loss: 1.160185, acc.: 50.00%] [G loss: 1.864354, acc.: 0.00%]
0 [D loss: 1.262532, acc.: 75.00%] [G loss: 1.320148, acc.: 50.00%]
0 [D loss: 1.127247, acc.: 50.00%] [G loss: 1.958980, acc.: 0.00%]
0 [D loss: 1.054820, acc.: 75.00%] [G loss: 1.339703, acc.: 50.00%]
0 [D loss: 1.070544, acc.: 100.00%] [G loss: 1.202253, acc.: 50.00%]
0 [D loss: 1.094277, acc.: 50.00%] [G loss: 1.517443, acc.: 50.00%]
0 [D loss: 1.158427, acc.: 75.00%] [G loss: 1.449952, acc.: 0.00%]
0 [D loss: 1.272630, acc.: 50.00%] [G loss: 1.317903, acc.: 50.00%]
0 [D loss: 1.158174, acc.: 50.00%] [G loss: 1.291218, acc.: 50.00%]
0 [D loss: 1.122268, acc.: 75.00%] [G loss: 1.711255, acc.: 50.00%]
0 [D loss: 1.031341, acc.: 100.00%] [G loss: 1.263629, acc.: 50.00%]
0 [D loss: 1.061359, acc.: 75.00%] [G loss: 1.460231, acc.: 50.00%]
0 [D loss: 1.255801, acc.: 50.00%] [G loss: 1.243761, acc.: 50.00%]
